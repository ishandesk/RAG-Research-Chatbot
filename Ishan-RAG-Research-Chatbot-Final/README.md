# AI-Assisted Research Chatbot (RAG + LangChain + FAISS)

**Author:** Ishan â€” Final Year Student, NIT Delhi  
**Role:** AI/ML Developer  
**Summary:** A Retrieval-Augmented Generation (RAG) chatbot that indexes domain documents and answers questions grounded in retrieved context. Built with LangChain, FAISS/Chroma, and OpenAI/HuggingFace backends. This is a polished, personally-authored restructure of my RAG project with consistent code style, docs, and evaluation hooks.

---

## âœ¨ Features
- ğŸ“¥ Ingestion pipeline for PDFs/TXT (chunking, embeddings)
- ğŸ” Vector search (FAISS/Chroma) with tunable `chunk_size`, `overlap`, `top_k`
- ğŸ§  LLM backends: OpenAI / HuggingFace (local) â€” easily switchable via `.env`
- ğŸ’¬ CLI and Streamlit UI for interactive Q&A
- ğŸ“Š Evaluation hooks to compare **RAG vs Base LLM**
- ğŸ§ª Clean, modular code with docstrings and standardized naming

---

## ğŸ§± Project Structure
```
Ishan-RAG-Research-Chatbot/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ LICENSE
â”œâ”€ .gitignore
â”œâ”€ data/                 # place your PDFs/TXTs here (optional)
â”œâ”€ artifacts/            # vector index persisted here (gitignored)
â”œâ”€ docs/                 # screenshots / notes
â””â”€ src/
   â”œâ”€ data_ingestion.py  # parse docs â†’ chunk â†’ embed â†’ index
   â”œâ”€ rag_pipeline.py    # retrieval + generation (CLI-friendly)
   â”œâ”€ app_streamlit.py   # simple Streamlit UI for Q&A
   â””â”€ utils/             # helper modules (prompts, loaders, etc.)
```

---

## âš™ï¸ Setup
```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

pip install -r requirements.txt
```

Create a `.env` file in the project root:
```dotenv
BACKEND=openai          # or 'hf' for HuggingFace
OPENAI_API_KEY=sk-...   # required if BACKEND=openai
OPENAI_MODEL=gpt-3.5-turbo
HF_MODEL=google/flan-t5-base
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

---

## ğŸ—ï¸ Build Index
Use the ingestion script to load documents, chunk them, and create a FAISS index:
```bash
python src/data_ingestion.py --input_dir data --chunk_size 600 --chunk_overlap 80 --store faiss
```

---

## ğŸ’¬ Ask Questions (CLI)
```bash
python src/rag_pipeline.py --question "What is retrieval-augmented generation?" --k 5
```

---

## ğŸ–¥ï¸ Streamlit UI
```bash
streamlit run src/app_streamlit.py
```

---

## ğŸ“Š Evaluation (RAG vs Base)
Use the placeholders in `src/` to evaluate semantic similarity against context as a proxy for factual grounding. (I keep evaluation lightweight for quick demos.)

---

## ğŸ§ª What I Learned
- Designing a **RAG** system that grounds LLM answers in retrieved context
- Practical **hyperparameter tuning** (chunk size/overlap, `top_k`, embedding model)
- Trade-offs between **FAISS vs Chroma** and **OpenAI vs HF** backends
- Building quick **Streamlit** demos and clean CLIs

---


MIT â€” see `LICENSE`.

---
